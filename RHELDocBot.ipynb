{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e7957e-3064-4330-879a-2e7415d11f94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install -U --quiet ipywidgets langchain langchain-community langchain-core langchainhub chromadb==0.4.15 pysqlite3-binary sentence-transformers pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc89bc4-53ae-4277-91b8-43fa136208d3",
   "metadata": {},
   "source": [
    "# Set Up The Model\n",
    "In this block, we install chromadb and other dependancies.  Chroma requires sqlite3 so that is imported as well.\n",
    "\n",
    "The LLM that is used is Mistral:Instruct that is hosted by an Ollama container running in OpenShift.\n",
    "\n",
    "HuggingFace Embeddings are used since they can be run locally and can be configured to take advantage of available GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d554fdc-e452-49e7-a6eb-74900cc66b2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "import chromadb\n",
    "\n",
    "import bs4\n",
    "import os.path\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "\n",
    "model = ChatOllama(model=\"mistral:instruct\",\n",
    "                   base_url=\"http://ollama-api-service.ollama-llm.svc.cluster.local:11434\",\n",
    "                   temperature = 0)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\",model_kwargs={'device': 'cuda'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04b2832-fa2f-44b0-8b9e-345eca336fff",
   "metadata": {},
   "source": [
    "# Gather Data, Chunk it and Store it in the vector store\n",
    "\n",
    "If the database is not present, then create it by downloading and chunking the files.  If it is present, then just load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3efcc1-7fe7-49e0-be93-985a6d53e76c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "persist_dir = \"db\"\n",
    "\n",
    "check_file = \"False\"\n",
    "\n",
    "path = 'db/chroma.sqlite3'\n",
    "\n",
    "check_file = os.path.isfile(path)\n",
    "\n",
    "if check_file is False:\n",
    "    urls = [\n",
    "        r'https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html-single/performing_a_standard_rhel_9_installation/index',\n",
    "        r'https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html-single/performing_an_advanced_rhel_9_installation/index',\n",
    "        r'https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html-single/configuring_basic_system_settings/index',\n",
    "        r'https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html-single/security_hardening/index',\n",
    "        r'https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html-single/composing_a_customized_rhel_system_image/index',\n",
    "        r'https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html-single/upgrading_from_rhel_8_to_rhel_9/index',\n",
    "        r'https://www.redhat.com/en/resources/red-hat-enterprise-linux-subscription-guide'\n",
    "    ]\n",
    "    \n",
    "    loader = WebBaseLoader(urls)\n",
    "    \n",
    "    docs = loader.load()\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1250, chunk_overlap=0)\n",
    "    \n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    \n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=embedding, persist_directory=\"db\")\n",
    "\n",
    "else:\n",
    "    \n",
    "    vectorstore = Chroma(persist_directory=persist_dir, embedding_function=embedding)\n",
    "    \n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c71b422-fc16-4834-b795-ecf4c08f4253",
   "metadata": {},
   "source": [
    "# Run the RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c3b0a6-eb40-4f97-b109-59a0c3060442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# Prompt\n",
    "rag_template = \"\"\"\n",
    "Given a question write an answer.\n",
    "Use only the supplied source docs.\n",
    "If you don't know the answer, just say that you don't know.  Do not fake the answer.\n",
    "If the answer is relevant, then ALWAYS include a \"SOURCES\" part in your answer.\n",
    "\n",
    "QUESTION: {question}\n",
    "=========\n",
    "{source_docs}\n",
    "=========\n",
    "ANSWER: \n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"Content: {doc.page_content}\\nSource: {doc.metadata['source']}\" for doc in docs\n",
    "    )\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(\n",
    "        source_docs=(lambda x: format_docs(x[\"source_docs\"]))\n",
    "    )\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain = RunnableParallel(\n",
    "    {\n",
    "        \"source_docs\": retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    ").assign(answer=rag_chain_from_docs)\n",
    "\n",
    "question = \"Can RHEL 9 be installed via a USB drive?\"\n",
    "\n",
    "results = (rag_chain.invoke(question))\n",
    "answer = results[\"answer\"]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6c00b1-14df-4bb6-8008-acc5f38bfc68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
